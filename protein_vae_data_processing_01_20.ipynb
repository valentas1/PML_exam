{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code is adapted from: https://github.com/OATML-Markslab/EVE"
      ],
      "metadata": {
        "id": "37YycL1bKwfs"
      },
      "id": "37YycL1bKwfs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMNAMgjic33D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import tqdm\n",
        "from scipy.special import erfinv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import statistics\n",
        "from tqdm import notebook\n"
      ],
      "id": "rMNAMgjic33D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jqqZ3_a_esM"
      },
      "outputs": [],
      "source": [
        "model_params = {   \n",
        "    \"encoder_parameters\": {\n",
        "        \"hidden_layers_sizes\"         :   [2000,1000,300],\n",
        "        \"z_dim\"                               :   2,\n",
        "        \"convolve_input\"                      :   False,\n",
        "        \"convolution_input_depth\"             :   40,\n",
        "        \"nonlinear_activation\"                :   \"relu\",\n",
        "        \"dropout_proba\"                       :   0.0\n",
        "    },\n",
        "    \"decoder_parameters\": {\n",
        "        \"hidden_layers_sizes\"         :   [300,1000,2000],\n",
        "        \"z_dim\"                               :   2,\n",
        "        \"bayesian_decoder\"                    :   False,\n",
        "        \"first_hidden_nonlinearity\"           :   \"relu\", \n",
        "        \"last_hidden_nonlinearity\"            :   \"relu\", \n",
        "        \"dropout_proba\"                       :   0.1,\n",
        "        \"convolve_output\"                     :   True,\n",
        "        \"convolution_output_depth\"            :   40, \n",
        "        \"include_temperature_scaler\"          :   True, \n",
        "        \"include_sparsity\"                    :   False, \n",
        "        \"num_tiles_sparsity\"                  :   0,\n",
        "        \"logit_sparsity_p\"                    :   0\n",
        "    },\n",
        "    \"training_parameters\": {\n",
        "        \"num_training_steps\"                :   150000,\n",
        "        \"learning_rate\"                     :   1e-4,\n",
        "        \"batch_size\"                        :   256,\n",
        "        \"annealing_warm_up\"                 :   0,\n",
        "        \"kl_latent_scale\"                   :   1.0,\n",
        "        \"kl_global_params_scale\"            :   1.0,\n",
        "        \"l2_regularization\"                 :   0.0,\n",
        "        \"use_lr_scheduler\"                  :   False,\n",
        "        \"use_validation_set\"                :   False,\n",
        "        \"validation_set_pct\"                :   0.2,\n",
        "        \"validation_freq\"                   :   1000,\n",
        "        \"log_training_info\"                 :   False,\n",
        "        \"log_training_freq\"                 :   1000,\n",
        "        \"save_model_params_freq\"            :   500000,\n",
        "        'model_checkpoint_location'         :   '.',\n",
        "        'calc_weights'                      :   False\n",
        "    }\n",
        "}"
      ],
      "id": "2jqqZ3_a_esM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd8e462a"
      },
      "outputs": [],
      "source": [
        "# FASTA parser requires Biopython\n",
        "try:\n",
        "    from Bio import SeqIO\n",
        "except:\n",
        "    !pip install biopython\n",
        "    from Bio import SeqIO\n",
        "    \n",
        "# Retrieve protein alignment file\n",
        "if not os.path.exists('BLAT_ECOLX_1_b0.5_labeled.fasta'):\n",
        "    !wget https://sid.erda.dk/share_redirect/a5PTfl88w0/BLAT_ECOLX_1_b0.5_labeled.fasta\n",
        "        \n",
        "# Retrieve file with experimental measurements\n",
        "if not os.path.exists('BLAT_ECOLX_Ranganathan2015.csv'):\n",
        "    !wget https://sid.erda.dk/share_redirect/a5PTfl88w0/BLAT_ECOLX_Ranganathan2015.csv\n",
        "        \n",
        "# Options\n",
        "batch_size = 16"
      ],
      "id": "cd8e462a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02a0af68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7f9bce-d5a6-4594-e246-1b2003f8d633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7844\n",
            "[1. 1. 1. ... 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "# Mapping from amino acids to integers\n",
        "aa1_to_index = {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6,\n",
        "                'I': 7, 'K': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12,\n",
        "                'Q': 13, 'R': 14, 'S': 15, 'T': 16, 'V': 17, 'W': 18,\n",
        "                'Y': 19, 'X':20, 'Z': 21, '-': 22}\n",
        "aa1 = \"ACDEFGHIKLMNPQRSTVWYXZ-\"\n",
        "\n",
        "phyla = ['Acidobacteria', 'Actinobacteria', 'Bacteroidetes',\n",
        "         'Chloroflexi', 'Cyanobacteria', 'Deinococcus-Thermus',\n",
        "         'Firmicutes', 'Fusobacteria', 'Proteobacteria', 'Other']\n",
        "\n",
        "def convert_to_one_hot_encoding(seq_list):\n",
        "    one_hot = np.array([aa1_to_index[aa] for aa in str(seq_list).upper().replace('.', '-')])\n",
        "    one_hot = F.one_hot(torch.from_numpy(one_hot))\n",
        "    target_one_hot = torch.zeros(one_hot.shape[0], 23)\n",
        "    target_one_hot[:, :one_hot.shape[1]] = one_hot\n",
        "    target_one_hot = target_one_hot[None, :, :]\n",
        "    return target_one_hot\n",
        "\n",
        "def get_data(data_filename, calc_weights=False, weights_similarity_threshold=0.8):\n",
        "    '''Create dataset from FASTA filename'''\n",
        "    ids = []\n",
        "    labels = []\n",
        "    seqs = []\n",
        "    label_re = re.compile(r'\\[([^\\]]*)\\]')\n",
        "    seqs_letters = []\n",
        "    for record in SeqIO.parse(data_filename, \"fasta\"):\n",
        "        ids.append(record.id)\n",
        "\n",
        "        seqs_letters.append(np.array([aa for aa in str(record.seq).upper().replace('.', '-')]))\n",
        "        seqs.append(np.array([aa1_to_index[aa] for aa in str(record.seq).upper().replace('.', '-')]))\n",
        "        \n",
        "        label = label_re.search(record.description).group(1)\n",
        "        # Only use most common classes\n",
        "        if label not in phyla:\n",
        "            label = 'Other'\n",
        "        labels.append(label)            \n",
        "    seqs = torch.from_numpy(np.vstack(seqs))\n",
        "    seqs_letters = np.vstack(seqs_letters)\n",
        "    labels = np.array(labels)\n",
        "    \n",
        "    phyla_lookup_table, phyla_idx = np.unique(labels, return_inverse=True)\n",
        "    # dataset = torch.utils.data.TensorDataset(*[seqs, torch.from_numpy(phyla_idx)])\n",
        "    \n",
        "    one_hot1 = F.one_hot(seqs[:len(seqs)//2].long()).bool()\n",
        "    one_hot2 = F.one_hot(seqs[len(seqs)//2:].long()).bool()\n",
        "    one_hot = torch.cat([one_hot1, one_hot2])\n",
        "    assert(len(seqs) == len(one_hot))\n",
        "    del one_hot1\n",
        "    del one_hot2\n",
        "    one_hot[seqs>19] = 0\n",
        "\n",
        "\n",
        "    # weights = None\n",
        "    if calc_weights is not False:\n",
        "\n",
        "        # Experiencing memory issues on colab for this code because pytorch doesn't\n",
        "        # allow one_hot directly to bool. Splitting in two and then merging.\n",
        "        # one_hot = F.one_hot(seqs.long()).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        flat_one_hot = one_hot.flatten(1)\n",
        "\n",
        "        weights = []\n",
        "        weight_batch_size = 1000\n",
        "        flat_one_hot = flat_one_hot.float()\n",
        "        for i in range(seqs.size(0) // weight_batch_size + 1):\n",
        "            x = flat_one_hot[i * weight_batch_size : (i + 1) * weight_batch_size]\n",
        "            similarities = torch.mm(x, flat_one_hot.T)\n",
        "            lengths = (seqs[i * weight_batch_size : (i + 1) * weight_batch_size] <=19).sum(1).unsqueeze(-1)\n",
        "            # w = 1.0 / (similarities / lengths).gt(weights_similarity_threshold).sum(1).float().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            w = 1.0 / (similarities / lengths).gt(weights_similarity_threshold).sum(1).float()\n",
        "            weights.append(w)\n",
        "            \n",
        "        weights = torch.cat(weights).numpy()\n",
        "        neff = weights.sum()\n",
        "\n",
        "    else:\n",
        "        weights = np.ones(seqs.shape[0])\n",
        "        neff = weights.sum()\n",
        "\n",
        "    one_hot = np.multiply(one_hot, 1).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # seq_len = one_hot.shape[1]\n",
        "    # alphabet_size = one_hot.shape[2]\n",
        "\n",
        "    # dataset_one_hot = torch.utils.data.TensorDataset(*[one_hot, torch.from_numpy(phyla_idx)])  \n",
        "    return seqs, seqs_letters, one_hot, torch.from_numpy(phyla_idx), weights, neff, phyla_lookup_table\n",
        "\n",
        "\n",
        "\n",
        "dataset, dataset_letters, dataset_one_hot, phyla_idx, weights, neff, phyla_lookup_table = get_data('BLAT_ECOLX_1_b0.5_labeled.fasta', calc_weights=model_params[\"training_parameters\"]['calc_weights'])\n",
        "print(dataset.shape[0])\n",
        "dataset_one_hot_tensor = torch.utils.data.TensorDataset(*[dataset_one_hot, phyla_idx])\n",
        "dataset_tensor = torch.utils.data.TensorDataset(*[dataset, phyla_idx])\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset_one_hot_tensor, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# print(phyla_lookup_table)\n",
        "print(weights)"
      ],
      "id": "02a0af68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0eeb48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "41ff06b1-b787-47a3-e672-278605d2cb05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b2686ff0-8a4e-46da-a8a4-5f0a5675638c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>D</th>\n",
              "      <th>G</th>\n",
              "      <th>F</th>\n",
              "      <th>I</th>\n",
              "      <th>K</th>\n",
              "      <th>M</th>\n",
              "      <th>L</th>\n",
              "      <th>N</th>\n",
              "      <th>Q</th>\n",
              "      <th>P</th>\n",
              "      <th>S</th>\n",
              "      <th>R</th>\n",
              "      <th>T</th>\n",
              "      <th>W</th>\n",
              "      <th>V</th>\n",
              "      <th>Y</th>\n",
              "      <th>H</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pos</th>\n",
              "      <th>WT</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th>H</th>\n",
              "      <td>-0.00978356</td>\n",
              "      <td>-0.41826</td>\n",
              "      <td>-0.279024</td>\n",
              "      <td>-0.181607</td>\n",
              "      <td>-0.0602417</td>\n",
              "      <td>-0.818487</td>\n",
              "      <td>-0.359191</td>\n",
              "      <td>0.0144696</td>\n",
              "      <td>-0.224781</td>\n",
              "      <td>-0.480347</td>\n",
              "      <td>-0.0430932</td>\n",
              "      <td>-0.135568</td>\n",
              "      <td>-1.01085</td>\n",
              "      <td>0.0361661</td>\n",
              "      <td>-0.00252626</td>\n",
              "      <td>-0.0671875</td>\n",
              "      <td>-1.34759</td>\n",
              "      <td>-0.026874</td>\n",
              "      <td>-0.885025</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <th>P</th>\n",
              "      <td>-1.6426</td>\n",
              "      <td>-0.364138</td>\n",
              "      <td>0.143258</td>\n",
              "      <td>-0.0284025</td>\n",
              "      <td>-0.969268</td>\n",
              "      <td>-0.199804</td>\n",
              "      <td>-0.0735238</td>\n",
              "      <td>0.13559</td>\n",
              "      <td>-0.0283657</td>\n",
              "      <td>-0.211869</td>\n",
              "      <td>0.0206405</td>\n",
              "      <td>-0.0282136</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.16013</td>\n",
              "      <td>0.054154</td>\n",
              "      <td>-0.0911999</td>\n",
              "      <td>-0.109139</td>\n",
              "      <td>0.045913</td>\n",
              "      <td>0.00174467</td>\n",
              "      <td>0.0457846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <th>E</th>\n",
              "      <td>0.0109131</td>\n",
              "      <td>-0.158233</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.0757852</td>\n",
              "      <td>0.0813101</td>\n",
              "      <td>-0.232106</td>\n",
              "      <td>-0.153907</td>\n",
              "      <td>0.0871198</td>\n",
              "      <td>-0.036441</td>\n",
              "      <td>-0.0581804</td>\n",
              "      <td>-0.0064688</td>\n",
              "      <td>0.0496907</td>\n",
              "      <td>-0.387232</td>\n",
              "      <td>-0.0395849</td>\n",
              "      <td>-0.220003</td>\n",
              "      <td>-0.135909</td>\n",
              "      <td>-0.44234</td>\n",
              "      <td>-0.0645674</td>\n",
              "      <td>-0.245436</td>\n",
              "      <td>0.0209168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <th>T</th>\n",
              "      <td>-1.45459</td>\n",
              "      <td>-2.41902</td>\n",
              "      <td>-2.41446</td>\n",
              "      <td>-2.29488</td>\n",
              "      <td>-2.35671</td>\n",
              "      <td>-2.60457</td>\n",
              "      <td>-0.280446</td>\n",
              "      <td>-1.42789</td>\n",
              "      <td>-1.8431</td>\n",
              "      <td>-0.765521</td>\n",
              "      <td>-2.48572</td>\n",
              "      <td>-1.6671</td>\n",
              "      <td>-1.79017</td>\n",
              "      <td>-1.39248</td>\n",
              "      <td>-2.37509</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-2.8417</td>\n",
              "      <td>0.0341893</td>\n",
              "      <td>-2.78913</td>\n",
              "      <td>-1.84954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <th>L</th>\n",
              "      <td>-0.202228</td>\n",
              "      <td>-1.95959</td>\n",
              "      <td>-1.72164</td>\n",
              "      <td>-2.71077</td>\n",
              "      <td>-1.4842</td>\n",
              "      <td>-0.720047</td>\n",
              "      <td>0.0173958</td>\n",
              "      <td>0.0695957</td>\n",
              "      <td>-0.0480697</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.42071</td>\n",
              "      <td>-0.222812</td>\n",
              "      <td>-2.19535</td>\n",
              "      <td>-1.2641</td>\n",
              "      <td>-0.0649357</td>\n",
              "      <td>-0.313656</td>\n",
              "      <td>-0.299738</td>\n",
              "      <td>0.0502655</td>\n",
              "      <td>-0.2186</td>\n",
              "      <td>-0.889277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <th>L</th>\n",
              "      <td>-0.103001</td>\n",
              "      <td>-0.617685</td>\n",
              "      <td>-2.95432</td>\n",
              "      <td>-2.52951</td>\n",
              "      <td>-2.41565</td>\n",
              "      <td>-0.663794</td>\n",
              "      <td>0.0786722</td>\n",
              "      <td>-2.57103</td>\n",
              "      <td>0.0126656</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-2.32138</td>\n",
              "      <td>-1.72228</td>\n",
              "      <td>-2.42792</td>\n",
              "      <td>-1.65506</td>\n",
              "      <td>-2.70359</td>\n",
              "      <td>-0.231658</td>\n",
              "      <td>-2.78162</td>\n",
              "      <td>-0.0205734</td>\n",
              "      <td>-2.60799</td>\n",
              "      <td>-3.01127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <th>I</th>\n",
              "      <td>-0.537631</td>\n",
              "      <td>-0.657012</td>\n",
              "      <td>-2.6017</td>\n",
              "      <td>-2.78886</td>\n",
              "      <td>-2.24781</td>\n",
              "      <td>-0.123746</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-2.24993</td>\n",
              "      <td>-0.0885522</td>\n",
              "      <td>-0.190349</td>\n",
              "      <td>-2.48529</td>\n",
              "      <td>-1.95522</td>\n",
              "      <td>-2.61388</td>\n",
              "      <td>-1.48797</td>\n",
              "      <td>-1.94825</td>\n",
              "      <td>-0.300995</td>\n",
              "      <td>-1.71315</td>\n",
              "      <td>-0.030749</td>\n",
              "      <td>-0.0821665</td>\n",
              "      <td>-2.07812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <th>K</th>\n",
              "      <td>-0.0389621</td>\n",
              "      <td>-0.91867</td>\n",
              "      <td>-0.0274808</td>\n",
              "      <td>-0.0614761</td>\n",
              "      <td>-0.0924616</td>\n",
              "      <td>-0.286851</td>\n",
              "      <td>-0.514835</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.107796</td>\n",
              "      <td>-0.0442595</td>\n",
              "      <td>-0.0745288</td>\n",
              "      <td>-0.0715558</td>\n",
              "      <td>-2.59327</td>\n",
              "      <td>-0.0202318</td>\n",
              "      <td>-0.04172</td>\n",
              "      <td>-0.090441</td>\n",
              "      <td>-0.449885</td>\n",
              "      <td>-0.261297</td>\n",
              "      <td>-0.111472</td>\n",
              "      <td>-0.00521766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <th>H</th>\n",
              "      <td>-0.465274</td>\n",
              "      <td>-0.251095</td>\n",
              "      <td>-0.338762</td>\n",
              "      <td>-1.53546</td>\n",
              "      <td>-0.509937</td>\n",
              "      <td>-0.149276</td>\n",
              "      <td>-2.72312</td>\n",
              "      <td>-0.271887</td>\n",
              "      <td>-0.226841</td>\n",
              "      <td>-0.553996</td>\n",
              "      <td>0.0240883</td>\n",
              "      <td>-0.141955</td>\n",
              "      <td>-2.72487</td>\n",
              "      <td>-0.160349</td>\n",
              "      <td>-0.157191</td>\n",
              "      <td>-0.113391</td>\n",
              "      <td>-0.0762315</td>\n",
              "      <td>-0.825731</td>\n",
              "      <td>-0.0946859</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <th>W</th>\n",
              "      <td>-3.1788</td>\n",
              "      <td>-2.98376</td>\n",
              "      <td>-2.54585</td>\n",
              "      <td>-2.85911</td>\n",
              "      <td>-3.22843</td>\n",
              "      <td>-0.780425</td>\n",
              "      <td>-2.4295</td>\n",
              "      <td>-2.56635</td>\n",
              "      <td>-1.69978</td>\n",
              "      <td>-2.24469</td>\n",
              "      <td>-2.58401</td>\n",
              "      <td>-2.69939</td>\n",
              "      <td>-2.79352</td>\n",
              "      <td>-2.96771</td>\n",
              "      <td>-2.70773</td>\n",
              "      <td>-2.80642</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-2.81384</td>\n",
              "      <td>-0.962361</td>\n",
              "      <td>-1.78274</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>263 rows Ã— 20 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2686ff0-8a4e-46da-a8a4-5f0a5675638c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2686ff0-8a4e-46da-a8a4-5f0a5675638c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2686ff0-8a4e-46da-a8a4-5f0a5675638c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 A         C          E  ...          V           Y           H\n",
              "pos WT                                   ...                                   \n",
              "0   H  -0.00978356  -0.41826  -0.279024  ...  -0.026874   -0.885025         NaN\n",
              "1   P      -1.6426 -0.364138   0.143258  ...   0.045913  0.00174467   0.0457846\n",
              "2   E    0.0109131 -0.158233        NaN  ... -0.0645674   -0.245436   0.0209168\n",
              "3   T     -1.45459  -2.41902   -2.41446  ...  0.0341893    -2.78913    -1.84954\n",
              "4   L    -0.202228  -1.95959   -1.72164  ...  0.0502655     -0.2186   -0.889277\n",
              "...            ...       ...        ...  ...        ...         ...         ...\n",
              "258 L    -0.103001 -0.617685   -2.95432  ... -0.0205734    -2.60799    -3.01127\n",
              "259 I    -0.537631 -0.657012    -2.6017  ...  -0.030749  -0.0821665    -2.07812\n",
              "260 K   -0.0389621  -0.91867 -0.0274808  ...  -0.261297   -0.111472 -0.00521766\n",
              "261 H    -0.465274 -0.251095  -0.338762  ...  -0.825731  -0.0946859         NaN\n",
              "262 W      -3.1788  -2.98376   -2.54585  ...   -2.81384   -0.962361    -1.78274\n",
              "\n",
              "[263 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "def read_experimental_data(filename, alignment_data, measurement_col_name = '2500', sequence_offset=0):\n",
        "    '''Read experimental data from csv file, and check that amino acid match those \n",
        "       in the first sequence of the alignment.\n",
        "       \n",
        "       measurement_col_name specifies which column in the csv file contains the experimental \n",
        "       observation. In our case, this is the one called 2500.\n",
        "       \n",
        "       sequence_offset is used in case there is an overall offset between the\n",
        "       indices in the two files.\n",
        "       '''\n",
        "    \n",
        "    measurement_df = pd.read_csv(filename, delimiter=',', usecols=['mutant', measurement_col_name])\n",
        "    wt_sequence, wt_label = alignment_data[0]\n",
        "    \n",
        "    zero_index = None\n",
        "    \n",
        "    experimental_data = {}\n",
        "    for idx, entry in measurement_df.iterrows():\n",
        "        mutant_from, position, mutant_to = entry['mutant'][:1],int(entry['mutant'][1:-1]),entry['mutant'][-1:]  \n",
        "        # Use index of first entry as offset (keep track of this in case \n",
        "        # there are index gaps in experimental data)\n",
        "        if zero_index is None:\n",
        "            zero_index = position\n",
        "            \n",
        "        # Corresponding position in our alignment\n",
        "        seq_position = position-zero_index+sequence_offset\n",
        "            \n",
        "        # Make sure that two two inputs agree on the indices: the \n",
        "        # amino acids in the first entry of the alignment should be \n",
        "        # identical to those in the experimental file.\n",
        "        assert mutant_from == aa1[wt_sequence[seq_position]]  \n",
        "        \n",
        "        if seq_position not in experimental_data:\n",
        "            experimental_data[seq_position] = {}\n",
        "        \n",
        "        # Check that there is only a single experimental value for mutant\n",
        "        assert mutant_to not in experimental_data[seq_position]\n",
        "        \n",
        "        experimental_data[seq_position]['pos'] = seq_position\n",
        "        experimental_data[seq_position]['WT'] = mutant_from\n",
        "        experimental_data[seq_position][mutant_to] = entry[measurement_col_name]\n",
        "    experimental_data = pd.DataFrame(experimental_data).transpose().set_index(['pos', 'WT'])\n",
        "    return experimental_data\n",
        "        \n",
        "        \n",
        "experimental_data = read_experimental_data(\"BLAT_ECOLX_Ranganathan2015.csv\", dataset_tensor)\n",
        "experimental_data"
      ],
      "id": "d0eeb48c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJDf_EVfrOD6"
      },
      "outputs": [],
      "source": [
        "def get_wt_sequence(experimental_data):\n",
        "    wt_list = [] \n",
        "    for (position, mutant_from), row in experimental_data.iterrows():\n",
        "        wt_list.append(mutant_from)\n",
        "    wt_seq = \"\".join(wt_list)\n",
        "    wt_one_hot = convert_to_one_hot_encoding(wt_seq)\n",
        "    # wt_one_hot = torch.from_numpy(wt_one_hot)\n",
        "    return wt_seq, wt_one_hot\n",
        "\n",
        "wt_seq, wt_one_hot = get_wt_sequence(experimental_data)\n",
        "\n",
        "def get_mutated_sequences(experimental_data, wt_seq):\n",
        "    mutated_list = []\n",
        "    mutation_list = []\n",
        "    exp_list = []\n",
        "    for (position, mutant_from), row in experimental_data.iterrows():\n",
        "        for mutant_to, exp_value in row.iteritems():\n",
        "            if mutant_from != mutant_to:\n",
        "                mut_seq = wt_seq[:position] + mutant_to + wt_seq[position+1:]\n",
        "                mut_seq_one_hot = convert_to_one_hot_encoding(mut_seq)\n",
        "                # mut_seq_one_hot = torch.from_numpy(mut_seq_one_hot)\n",
        "                mutated_list.append(mut_seq_one_hot)\n",
        "                mutation_list.append((position, mutant_to))\n",
        "                exp_list.append(exp_value) \n",
        "    return(mutated_list, mutation_list, exp_list)\n",
        "\n",
        "mut_one_hot, pos_info, exp_list = get_mutated_sequences(experimental_data, wt_seq)\n",
        "# print(torch.stack(mut_one_hot).squeeze().shape)\n",
        "test_dataloader = torch.utils.data.DataLoader(torch.stack(mut_one_hot).squeeze(), batch_size=model_params['training_parameters']['batch_size'], shuffle=False)\n"
      ],
      "id": "jJDf_EVfrOD6"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_basic_model(experimental_data, dataset_letters, wt_seq):\n",
        "    val_list = []\n",
        "    exp_list = []\n",
        "    for (position, mutant_from), row in experimental_data.iterrows():\n",
        "        # print(mutant_from)\n",
        "        col= dataset_letters[:,position]\n",
        "        counter_wt = np.sum(weights[col==str(mutant_from)])\n",
        "        # print(counter_wt)\n",
        "        for mutant_to, exp_value in row.iteritems():\n",
        "            if mutant_from != mutant_to:\n",
        "                counter_mut = np.sum(weights[col==str(mutant_to)])\n",
        "                val = counter_mut/counter_wt\n",
        "                val_list.append(val)\n",
        "                exp_list.append(exp_value)\n",
        "    return(val_list, exp_list)\n",
        "\n",
        "val_list, exp_list = get_basic_model(experimental_data, dataset_letters, wt_seq)"
      ],
      "metadata": {
        "id": "hhC-zUS28ICH"
      },
      "id": "hhC-zUS28ICH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spearmanr(val_list, exp_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATC4QemhWzjk",
        "outputId": "91685c97-6d20-4fb0-a072-de7986389f9a"
      },
      "id": "ATC4QemhWzjk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpearmanrResult(correlation=0.5637427310119946, pvalue=0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8c79kmcXObU"
      },
      "outputs": [],
      "source": [
        "class data_class: \n",
        "    def __init__(self,\n",
        "                  data_filename,\n",
        "                  calc_weights):\n",
        "        dataset_digits, _, dataset_one_hot, phyla_idx, weights, neff, phyla_lookup_table = get_data(data_filename, calc_weights)\n",
        "        self.seq_len = dataset_one_hot.shape[1]\n",
        "        self.alphabet_size = dataset_one_hot.shape[2]\n",
        "        self.Neff = neff\n",
        "        self.one_hot_encoding = dataset_one_hot\n",
        "        self.weights = weights\n",
        "\n",
        "                      "
      ],
      "id": "G8c79kmcXObU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILRttLtsxkfx"
      },
      "outputs": [],
      "source": [
        "class VAE_MLP_encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP encoder class for the VAE model.\n",
        "    \"\"\"\n",
        "    def __init__(self,params):\n",
        "        \"\"\"\n",
        "        Required input parameters:\n",
        "        - seq_len: (Int) Sequence length of sequence alignment\n",
        "        - alphabet_size: (Int) Alphabet size of sequence alignment (will be driven by the data helper object)\n",
        "        - hidden_layers_sizes: (List) List of sizes of DNN linear layers\n",
        "        - z_dim: (Int) Size of latent space\n",
        "        - convolve_input: (Bool) Whether to perform 1d convolution on input (kernel size 1, stide 1)\n",
        "        - convolution_depth: (Int) Size of the 1D-convolution on input\n",
        "        - nonlinear_activation: (Str) Type of non-linear activation to apply on each hidden layer\n",
        "        - dropout_proba: (Float) Dropout probability applied on all hidden layers. If 0.0 then no dropout applied\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.seq_len = params['seq_len']\n",
        "        self.alphabet_size = params['alphabet_size']\n",
        "        self.hidden_layers_sizes = params['hidden_layers_sizes']\n",
        "        self.z_dim = params['z_dim']\n",
        "        self.convolve_input = params['convolve_input']\n",
        "        self.convolution_depth = params['convolution_input_depth']\n",
        "        self.dropout_proba = params['dropout_proba']\n",
        "\n",
        "        self.mu_bias_init = 0.1\n",
        "        self.log_var_bias_init = -10.0\n",
        "\n",
        "        #Convolving input with kernels of size 1 to capture potential similarities across amino acids when encoding sequences\n",
        "        if self.convolve_input:\n",
        "            self.input_convolution = nn.Conv1d(in_channels=self.alphabet_size,out_channels=self.convolution_depth,kernel_size=1,stride=1,bias=False)\n",
        "            self.channel_size = self.convolution_depth\n",
        "        else:\n",
        "            self.channel_size = self.alphabet_size\n",
        "\n",
        "        self.hidden_layers=torch.nn.ModuleDict()\n",
        "        for layer_index in range(len(self.hidden_layers_sizes)):\n",
        "            if layer_index==0:\n",
        "                self.hidden_layers[str(layer_index)] = nn.Linear((self.channel_size*self.seq_len),self.hidden_layers_sizes[layer_index])\n",
        "                nn.init.constant_(self.hidden_layers[str(layer_index)].bias, self.mu_bias_init)\n",
        "            else:\n",
        "                self.hidden_layers[str(layer_index)] = nn.Linear(self.hidden_layers_sizes[layer_index-1],self.hidden_layers_sizes[layer_index])\n",
        "                nn.init.constant_(self.hidden_layers[str(layer_index)].bias, self.mu_bias_init)\n",
        "        \n",
        "        self.fc_mean = nn.Linear(self.hidden_layers_sizes[-1],self.z_dim)\n",
        "        nn.init.constant_(self.fc_mean.bias, self.mu_bias_init)\n",
        "        self.fc_log_var = nn.Linear(self.hidden_layers_sizes[-1],self.z_dim)\n",
        "        nn.init.constant_(self.fc_log_var.bias, self.log_var_bias_init)\n",
        "\n",
        "        # set up non-linearity\n",
        "        if params['nonlinear_activation'] == 'relu':\n",
        "            self.nonlinear_activation = nn.ReLU()\n",
        "        elif params['nonlinear_activation'] == 'tanh':\n",
        "            self.nonlinear_activation = nn.Tanh()\n",
        "        elif params['nonlinear_activation'] == 'sigmoid':\n",
        "            self.nonlinear_activation = nn.Sigmoid()\n",
        "        elif params['nonlinear_activation'] == 'elu':\n",
        "            self.nonlinear_activation = nn.ELU()\n",
        "        elif params['nonlinear_activation'] == 'linear':\n",
        "            self.nonlinear_activation = nn.Identity()\n",
        "        \n",
        "        if self.dropout_proba > 0.0:\n",
        "            self.dropout_layer = nn.Dropout(p=self.dropout_proba)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.dropout_proba > 0.0:\n",
        "            x = self.dropout_layer(x)\n",
        "\n",
        "        if self.convolve_input:\n",
        "            x = x.permute(0,2,1) \n",
        "            x = self.input_convolution(x)\n",
        "            x = x.view(-1,self.seq_len*self.channel_size)\n",
        "        else:\n",
        "            x = x.view(-1,self.seq_len*self.channel_size) \n",
        "        \n",
        "        for layer_index in range(len(self.hidden_layers_sizes)):\n",
        "            x = self.nonlinear_activation(self.hidden_layers[str(layer_index)](x))\n",
        "            if self.dropout_proba > 0.0:\n",
        "                x = self.dropout_layer(x)\n",
        "\n",
        "        z_mean = self.fc_mean(x)\n",
        "        z_log_var = self.fc_log_var(x)\n",
        "\n",
        "        return z_mean, z_log_var\n",
        "\n",
        "class VAE_Standard_MLP_decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard MLP decoder class for the VAE model.\n",
        "    \"\"\"\n",
        "    def __init__(self, params):\n",
        "        \"\"\"\n",
        "        Required input parameters:\n",
        "        - seq_len: (Int) Sequence length of sequence alignment\n",
        "        - alphabet_size: (Int) Alphabet size of sequence alignment (will be driven by the data helper object)\n",
        "        - hidden_layers_sizes: (List) List of the sizes of the hidden layers (all DNNs)\n",
        "        - z_dim: (Int) Dimension of latent space\n",
        "        - first_hidden_nonlinearity: (Str) Type of non-linear activation applied on the first (set of) hidden layer(s)\n",
        "        - last_hidden_nonlinearity: (Str) Type of non-linear activation applied on the very last hidden layer (pre-sparsity)\n",
        "        - dropout_proba: (Float) Dropout probability applied on all hidden layers. If 0.0 then no dropout applied\n",
        "        - convolve_output: (Bool) Whether to perform 1d convolution on output (kernel size 1, stide 1)\n",
        "        - convolution_depth: (Int) Size of the 1D-convolution on output\n",
        "        - include_temperature_scaler: (Bool) Whether we apply the global temperature scaler\n",
        "        - include_sparsity: (Bool) Whether we use the sparsity inducing scheme on the output from the last hidden layer\n",
        "        - num_tiles_sparsity: (Int) Number of tiles to use in the sparsity inducing scheme (the more the tiles, the stronger the sparsity)\n",
        "        - bayesian_decoder: (Bool) Whether the decoder is bayesian or not\n",
        "        \"\"\"\n",
        "        super().__init__()        \n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.seq_len = params['seq_len']\n",
        "        self.alphabet_size = params['alphabet_size']\n",
        "        self.hidden_layers_sizes = params['hidden_layers_sizes']\n",
        "        self.z_dim = params['z_dim']\n",
        "        self.bayesian_decoder = False\n",
        "        self.dropout_proba = params['dropout_proba']\n",
        "        self.convolve_output = params['convolve_output']\n",
        "        self.convolution_depth = params['convolution_output_depth']\n",
        "        self.include_temperature_scaler = params['include_temperature_scaler']\n",
        "        self.include_sparsity = params['include_sparsity']\n",
        "        self.num_tiles_sparsity = params['num_tiles_sparsity']\n",
        "\n",
        "        self.mu_bias_init = 0.1\n",
        "\n",
        "        self.hidden_layers=nn.ModuleDict()\n",
        "        for layer_index in range(len(self.hidden_layers_sizes)):\n",
        "            if layer_index==0:\n",
        "                self.hidden_layers[str(layer_index)] = nn.Linear(self.z_dim, self.hidden_layers_sizes[layer_index])\n",
        "                nn.init.constant_(self.hidden_layers[str(layer_index)].bias, self.mu_bias_init)\n",
        "            else:\n",
        "                self.hidden_layers[str(layer_index)] = nn.Linear(self.hidden_layers_sizes[layer_index-1],self.hidden_layers_sizes[layer_index])\n",
        "                nn.init.constant_(self.hidden_layers[str(layer_index)].bias, self.mu_bias_init)\n",
        "\n",
        "        if params['first_hidden_nonlinearity'] == 'relu':\n",
        "            self.first_hidden_nonlinearity = nn.ReLU()\n",
        "        elif params['first_hidden_nonlinearity'] == 'tanh':\n",
        "            self.first_hidden_nonlinearity = nn.Tanh()\n",
        "        elif params['first_hidden_nonlinearity'] == 'sigmoid':\n",
        "            self.first_hidden_nonlinearity = nn.Sigmoid()\n",
        "        elif params['first_hidden_nonlinearity'] == 'elu':\n",
        "            self.first_hidden_nonlinearity = nn.ELU()\n",
        "        elif params['first_hidden_nonlinearity'] == 'linear':\n",
        "            self.first_hidden_nonlinearity = nn.Identity()\n",
        "        \n",
        "        if params['last_hidden_nonlinearity'] == 'relu':\n",
        "            self.last_hidden_nonlinearity = nn.ReLU()\n",
        "        elif params['last_hidden_nonlinearity'] == 'tanh':\n",
        "            self.last_hidden_nonlinearity = nn.Tanh()\n",
        "        elif params['last_hidden_nonlinearity'] == 'sigmoid':\n",
        "            self.last_hidden_nonlinearity = nn.Sigmoid()\n",
        "        elif params['last_hidden_nonlinearity'] == 'elu':\n",
        "            self.last_hidden_nonlinearity = nn.ELU()\n",
        "        elif params['last_hidden_nonlinearity'] == 'linear':\n",
        "            self.last_hidden_nonlinearity = nn.Identity()\n",
        "\n",
        "        if self.dropout_proba > 0.0:\n",
        "            self.dropout_layer = nn.Dropout(p=self.dropout_proba)\n",
        "\n",
        "        if self.convolve_output:\n",
        "            self.output_convolution = nn.Conv1d(in_channels=self.convolution_depth,out_channels=self.alphabet_size,kernel_size=1,stride=1,bias=False)\n",
        "            self.channel_size = self.convolution_depth\n",
        "        else:\n",
        "            self.channel_size = self.alphabet_size\n",
        "        \n",
        "        if self.include_sparsity:\n",
        "            self.sparsity_weight = nn.Parameter(torch.randn(int(self.hidden_layers_sizes[-1]/self.num_tiles_sparsity), self.seq_len))\n",
        "\n",
        "        self.W_out = nn.Parameter(torch.zeros(self.channel_size * self.seq_len,self.hidden_layers_sizes[-1]))\n",
        "        nn.init.xavier_normal_(self.W_out) #Initialize weights with Glorot initialization\n",
        "        self.b_out = nn.Parameter(torch.zeros(self.alphabet_size * self.seq_len))\n",
        "        nn.init.constant_(self.b_out, self.mu_bias_init)\n",
        "        \n",
        "        if self.include_temperature_scaler:\n",
        "            self.temperature_scaler = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, z):\n",
        "        batch_size = z.shape[0]\n",
        "        if self.dropout_proba > 0.0:\n",
        "            x = self.dropout_layer(z)\n",
        "        else:\n",
        "            x=z\n",
        "\n",
        "        for layer_index in range(len(self.hidden_layers_sizes)-1):\n",
        "            x = self.first_hidden_nonlinearity(self.hidden_layers[str(layer_index)](x))\n",
        "            if self.dropout_proba > 0.0:\n",
        "                x = self.dropout_layer(x)\n",
        "\n",
        "        x = self.last_hidden_nonlinearity(self.hidden_layers[str(len(self.hidden_layers_sizes)-1)](x)) #of size (batch_size,H)\n",
        "        if self.dropout_proba > 0.0:\n",
        "            x = self.dropout_layer(x)\n",
        "\n",
        "        W_out = self.W_out.data\n",
        "\n",
        "        if self.convolve_output:\n",
        "            W_out = torch.mm(W_out.view(self.seq_len * self.hidden_layers_sizes[-1], self.channel_size), \n",
        "                                    self.output_convolution.weight.view(self.channel_size,self.alphabet_size))\n",
        "\n",
        "        if self.include_sparsity:\n",
        "            sparsity_tiled = self.sparsity_weight.repeat(self.num_tiles_sparsity,1) #of size (H,seq_len)\n",
        "            sparsity_tiled = nn.Sigmoid()(sparsity_tiled).unsqueeze(2) #of size (H,seq_len,1)\n",
        "            W_out = W_out.view(self.hidden_layers_sizes[-1], self.seq_len, self.alphabet_size) * sparsity_tiled\n",
        "\n",
        "        W_out = W_out.view(self.seq_len * self.alphabet_size, self.hidden_layers_sizes[-1])\n",
        "\n",
        "        x = F.linear(x, weight=W_out, bias=self.b_out)\n",
        "\n",
        "        if self.include_temperature_scaler:\n",
        "            x = torch.log(1.0+torch.exp(self.temperature_scaler)) * x\n",
        "\n",
        "        x = x.view(batch_size, self.seq_len, self.alphabet_size)\n",
        "        x_recon_log = F.log_softmax(x, dim=-1) #of shape (batch_size, seq_len, alphabet)\n",
        "\n",
        "        return x_recon_log\n",
        "\n",
        "class VAE_Bayesian_MLP_decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Bayesian MLP decoder class for the VAE model.\n",
        "    \"\"\"\n",
        "    def __init__(self, params):\n",
        "        \"\"\"\n",
        "        Required input parameters:\n",
        "        - seq_len: (Int) Sequence length of sequence alignment\n",
        "        - alphabet_size: (Int) Alphabet size of sequence alignment (will be driven by the data helper object)\n",
        "        - hidden_layers_sizes: (List) List of the sizes of the hidden layers (all DNNs)\n",
        "        - z_dim: (Int) Dimension of latent space\n",
        "        - first_hidden_nonlinearity: (Str) Type of non-linear activation applied on the first (set of) hidden layer(s)\n",
        "        - last_hidden_nonlinearity: (Str) Type of non-linear activation applied on the very last hidden layer (pre-sparsity)\n",
        "        - dropout_proba: (Float) Dropout probability applied on all hidden layers. If 0.0 then no dropout applied\n",
        "        - convolve_output: (Bool) Whether to perform 1d convolution on output (kernel size 1, stide 1)\n",
        "        - convolution_depth: (Int) Size of the 1D-convolution on output\n",
        "        - include_temperature_scaler: (Bool) Whether we apply the global temperature scaler\n",
        "        - include_sparsity: (Bool) Whether we use the sparsity inducing scheme on the output from the last hidden layer\n",
        "        - num_tiles_sparsity: (Int) Number of tiles to use in the sparsity inducing scheme (the more the tiles, the stronger the sparsity)\n",
        "        - bayesian_decoder: (Bool) Whether the decoder is bayesian or not\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.seq_len = params['seq_len']\n",
        "        self.alphabet_size = params['alphabet_size']\n",
        "        self.hidden_layers_sizes = params['hidden_layers_sizes']\n",
        "        self.z_dim = params['z_dim']\n",
        "        self.bayesian_decoder = True\n",
        "        self.dropout_proba = params['dropout_proba']\n",
        "        self.convolve_output = params['convolve_output']\n",
        "        self.convolution_depth = params['convolution_output_depth']\n",
        "        self.include_temperature_scaler = params['include_temperature_scaler']\n",
        "        self.include_sparsity = params['include_sparsity']\n",
        "        self.num_tiles_sparsity = params['num_tiles_sparsity']\n",
        "\n",
        "        self.mu_bias_init = 0.1\n",
        "        self.logvar_init = -10.0\n",
        "        self.logit_scale_p = 0.001\n",
        "        \n",
        "        self.hidden_layers_mean=nn.ModuleDict()\n",
        "        self.hidden_layers_log_var=nn.ModuleDict()\n",
        "        for layer_index in range(len(self.hidden_layers_sizes)):\n",
        "            if layer_index==0:\n",
        "                self.hidden_layers_mean[str(layer_index)] = nn.Linear(self.z_dim, self.hidden_layers_sizes[layer_index])\n",
        "                self.hidden_layers_log_var[str(layer_index)] = nn.Linear(self.z_dim, self.hidden_layers_sizes[layer_index])\n",
        "                nn.init.constant_(self.hidden_layers_mean[str(layer_index)].bias, self.mu_bias_init)\n",
        "                nn.init.constant_(self.hidden_layers_log_var[str(layer_index)].weight, self.logvar_init)\n",
        "                nn.init.constant_(self.hidden_layers_log_var[str(layer_index)].bias, self.logvar_init)\n",
        "            else:\n",
        "                self.hidden_layers_mean[str(layer_index)] = nn.Linear(self.hidden_layers_sizes[layer_index-1],self.hidden_layers_sizes[layer_index])\n",
        "                self.hidden_layers_log_var[str(layer_index)] = nn.Linear(self.hidden_layers_sizes[layer_index-1],self.hidden_layers_sizes[layer_index])\n",
        "                nn.init.constant_(self.hidden_layers_mean[str(layer_index)].bias, self.mu_bias_init)\n",
        "                nn.init.constant_(self.hidden_layers_log_var[str(layer_index)].weight, self.logvar_init)\n",
        "                nn.init.constant_(self.hidden_layers_log_var[str(layer_index)].bias, self.logvar_init)\n",
        "\n",
        "        if params['first_hidden_nonlinearity'] == 'relu':\n",
        "            self.first_hidden_nonlinearity = nn.ReLU()\n",
        "        elif params['first_hidden_nonlinearity'] == 'tanh':\n",
        "            self.first_hidden_nonlinearity = nn.Tanh()\n",
        "        elif params['first_hidden_nonlinearity'] == 'sigmoid':\n",
        "            self.first_hidden_nonlinearity = nn.Sigmoid()\n",
        "        elif params['first_hidden_nonlinearity'] == 'elu':\n",
        "            self.first_hidden_nonlinearity = nn.ELU()\n",
        "        elif params['first_hidden_nonlinearity'] == 'linear':\n",
        "            self.first_hidden_nonlinearity = nn.Identity()\n",
        "        \n",
        "        if params['last_hidden_nonlinearity'] == 'relu':\n",
        "            self.last_hidden_nonlinearity = nn.ReLU()\n",
        "        elif params['last_hidden_nonlinearity'] == 'tanh':\n",
        "            self.last_hidden_nonlinearity = nn.Tanh()\n",
        "        elif params['last_hidden_nonlinearity'] == 'sigmoid':\n",
        "            self.last_hidden_nonlinearity = nn.Sigmoid()\n",
        "        elif params['last_hidden_nonlinearity'] == 'elu':\n",
        "            self.last_hidden_nonlinearity = nn.ELU()\n",
        "        elif params['last_hidden_nonlinearity'] == 'linear':\n",
        "            self.last_hidden_nonlinearity = nn.Identity()\n",
        "\n",
        "        if self.dropout_proba > 0.0:\n",
        "            self.dropout_layer = nn.Dropout(p=self.dropout_proba)\n",
        "\n",
        "        if self.convolve_output:\n",
        "            self.output_convolution_mean = nn.Conv1d(in_channels=self.convolution_depth,out_channels=self.alphabet_size,kernel_size=1,stride=1,bias=False)\n",
        "            self.output_convolution_log_var = nn.Conv1d(in_channels=self.convolution_depth,out_channels=self.alphabet_size,kernel_size=1,stride=1,bias=False)\n",
        "            nn.init.constant_(self.output_convolution_log_var.weight, self.logvar_init)\n",
        "            self.channel_size = self.convolution_depth\n",
        "        else:\n",
        "            self.channel_size = self.alphabet_size\n",
        "        \n",
        "        if self.include_sparsity:\n",
        "            self.sparsity_weight_mean = nn.Parameter(torch.zeros(int(self.hidden_layers_sizes[-1]/self.num_tiles_sparsity), self.seq_len))\n",
        "            self.sparsity_weight_log_var = nn.Parameter(torch.ones(int(self.hidden_layers_sizes[-1]/self.num_tiles_sparsity), self.seq_len))\n",
        "            nn.init.constant_(self.sparsity_weight_log_var, self.logvar_init)\n",
        "\n",
        "        self.last_hidden_layer_weight_mean = nn.Parameter(torch.zeros(self.channel_size * self.seq_len,self.hidden_layers_sizes[-1]))\n",
        "        self.last_hidden_layer_weight_log_var = nn.Parameter(torch.zeros(self.channel_size * self.seq_len,self.hidden_layers_sizes[-1]))\n",
        "        nn.init.xavier_normal_(self.last_hidden_layer_weight_mean) #Glorot initialization\n",
        "        nn.init.constant_(self.last_hidden_layer_weight_log_var, self.logvar_init)\n",
        "\n",
        "        self.last_hidden_layer_bias_mean = nn.Parameter(torch.zeros(self.alphabet_size * self.seq_len))\n",
        "        self.last_hidden_layer_bias_log_var = nn.Parameter(torch.zeros(self.alphabet_size * self.seq_len))\n",
        "        nn.init.constant_(self.last_hidden_layer_bias_mean, self.mu_bias_init)\n",
        "        nn.init.constant_(self.last_hidden_layer_bias_log_var, self.logvar_init)\n",
        "        \n",
        "        if self.include_temperature_scaler:\n",
        "            self.temperature_scaler_mean = nn.Parameter(torch.ones(1))\n",
        "            self.temperature_scaler_log_var = nn.Parameter(torch.ones(1) * self.logvar_init) \n",
        "            \n",
        "    def sampler(self, mean, log_var):\n",
        "        \"\"\"\n",
        "        Samples a latent vector via reparametrization trick\n",
        "        \"\"\"\n",
        "        eps = torch.randn_like(mean).to(self.device)\n",
        "        z = torch.exp(0.5*log_var) * eps + mean\n",
        "        return z\n",
        "\n",
        "    def forward(self, z):\n",
        "        batch_size = z.shape[0]\n",
        "        if self.dropout_proba > 0.0:\n",
        "            x = self.dropout_layer(z)\n",
        "        else:\n",
        "            x = z\n",
        "\n",
        "        for layer_index in range(len(self.hidden_layers_sizes)-1):\n",
        "            layer_i_weight = self.sampler(self.hidden_layers_mean[str(layer_index)].weight, self.hidden_layers_log_var[str(layer_index)].weight)\n",
        "            layer_i_bias = self.sampler(self.hidden_layers_mean[str(layer_index)].bias, self.hidden_layers_log_var[str(layer_index)].bias)\n",
        "            x = self.first_hidden_nonlinearity(F.linear(x, weight=layer_i_weight, bias=layer_i_bias))\n",
        "            if self.dropout_proba > 0.0:\n",
        "                x = self.dropout_layer(x)\n",
        "\n",
        "        last_index = len(self.hidden_layers_sizes)-1\n",
        "        last_layer_weight = self.sampler(self.hidden_layers_mean[str(last_index)].weight, self.hidden_layers_log_var[str(last_index)].weight)\n",
        "        last_layer_bias = self.sampler(self.hidden_layers_mean[str(last_index)].bias, self.hidden_layers_log_var[str(last_index)].bias)\n",
        "        x = self.last_hidden_nonlinearity(F.linear(x, weight=last_layer_weight, bias=last_layer_bias))\n",
        "        if self.dropout_proba > 0.0:\n",
        "            x = self.dropout_layer(x)\n",
        "\n",
        "        W_out = self.sampler(self.last_hidden_layer_weight_mean, self.last_hidden_layer_weight_log_var)\n",
        "        b_out = self.sampler(self.last_hidden_layer_bias_mean, self.last_hidden_layer_bias_log_var)\n",
        "\n",
        "        if self.convolve_output:\n",
        "            output_convolution_weight = self.sampler(self.output_convolution_mean.weight, self.output_convolution_log_var.weight)\n",
        "            W_out = torch.mm(W_out.view(self.seq_len * self.hidden_layers_sizes[-1], self.channel_size), \n",
        "                                    output_convolution_weight.view(self.channel_size,self.alphabet_size)) #product of size (H * seq_len, alphabet)\n",
        "            \n",
        "        if self.include_sparsity:\n",
        "            sparsity_weights = self.sampler(self.sparsity_weight_mean,self.sparsity_weight_log_var)\n",
        "            sparsity_tiled = sparsity_weights.repeat(self.num_tiles_sparsity,1) \n",
        "            sparsity_tiled = nn.Sigmoid()(sparsity_tiled).unsqueeze(2) \n",
        "\n",
        "            W_out = W_out.view(self.hidden_layers_sizes[-1], self.seq_len, self.alphabet_size) * sparsity_tiled\n",
        "        \n",
        "        W_out = W_out.view(self.seq_len * self.alphabet_size, self.hidden_layers_sizes[-1])\n",
        "        \n",
        "        x = F.linear(x, weight=W_out, bias=b_out)\n",
        "\n",
        "        if self.include_temperature_scaler:\n",
        "            temperature_scaler = self.sampler(self.temperature_scaler_mean,self.temperature_scaler_log_var)\n",
        "            x = torch.log(1.0+torch.exp(temperature_scaler)) * x\n",
        "\n",
        "        x = x.view(batch_size, self.seq_len, self.alphabet_size)\n",
        "        x_recon_log = F.log_softmax(x, dim=-1) #of shape (batch_size, seq_len, alphabet)\n",
        "\n",
        "        return x_recon_log\n"
      ],
      "id": "ILRttLtsxkfx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhoTmv9HxmCE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from . import VAE_encoder, VAE_decoder\n",
        "\n",
        "class VAE_model(nn.Module):\n",
        "    \"\"\"\n",
        "    Class for the VAE model with estimation of weights distribution parameters via Mean-Field VI.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "            model_name,\n",
        "            data,\n",
        "            encoder_parameters,\n",
        "            decoder_parameters,\n",
        "            random_seed\n",
        "            ):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.model_name = model_name\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.dtype = torch.float32\n",
        "        self.random_seed = random_seed\n",
        "        torch.manual_seed(random_seed)\n",
        "        \n",
        "        self.seq_len = data.seq_len\n",
        "        self.alphabet_size = data.alphabet_size\n",
        "        self.Neff = data.Neff\n",
        "\n",
        "        encoder_parameters['seq_len'] = self.seq_len\n",
        "        encoder_parameters['alphabet_size'] = self.alphabet_size\n",
        "        decoder_parameters['seq_len'] = self.seq_len\n",
        "        decoder_parameters['alphabet_size'] = self.alphabet_size\n",
        "        \n",
        "        self.encoder = VAE_MLP_encoder(params=encoder_parameters)\n",
        "        if decoder_parameters['bayesian_decoder']:\n",
        "            self.decoder = VAE_Bayesian_MLP_decoder(params=decoder_parameters)\n",
        "        else:\n",
        "            self.decoder = VAE_Standard_MLP_decoder(params=decoder_parameters)\n",
        "        self.logit_sparsity_p = decoder_parameters['logit_sparsity_p']\n",
        "        \n",
        "\n",
        "\n",
        "    def sample_latent(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        Samples a latent vector via reparametrization trick\n",
        "        \"\"\"\n",
        "        eps = torch.randn_like(mu).to(self.device)\n",
        "        z = torch.exp(0.5*log_var) * eps + mu\n",
        "        return z\n",
        "\n",
        "    def KLD_diag_gaussians(self, mu, logvar, p_mu, p_logvar):\n",
        "        \"\"\"\n",
        "        KL divergence between diagonal gaussian with prior diagonal gaussian.\n",
        "        \"\"\"\n",
        "        KLD = 0.5 * (p_logvar - logvar) + 0.5 * (torch.exp(logvar) + torch.pow(mu-p_mu,2)) / (torch.exp(p_logvar)+1e-20) - 0.5\n",
        "\n",
        "        return torch.sum(KLD)\n",
        "\n",
        "    def annealing_factor(self, annealing_warm_up, training_step):\n",
        "        \"\"\"\n",
        "        Annealing schedule of KL to focus on reconstruction error in early stages of training\n",
        "        \"\"\"\n",
        "        if training_step < annealing_warm_up:\n",
        "            return training_step/annealing_warm_up\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    def KLD_global_parameters(self):\n",
        "        \"\"\"\n",
        "        KL divergence between the variational distributions and the priors (for the decoder weights).\n",
        "        \"\"\"\n",
        "        KLD_decoder_params = 0.0\n",
        "        zero_tensor = torch.tensor(0.0).to(self.device) \n",
        "        \n",
        "        for layer_index in range(len(self.decoder.hidden_layers_sizes)):\n",
        "            for param_type in ['weight','bias']:\n",
        "                KLD_decoder_params += self.KLD_diag_gaussians(\n",
        "                                    self.decoder.state_dict(keep_vars=True)['hidden_layers_mean.'+str(layer_index)+'.'+param_type].flatten(),\n",
        "                                    self.decoder.state_dict(keep_vars=True)['hidden_layers_log_var.'+str(layer_index)+'.'+param_type].flatten(),\n",
        "                                    zero_tensor,\n",
        "                                    zero_tensor\n",
        "                )\n",
        "                \n",
        "        for param_type in ['weight','bias']:\n",
        "                KLD_decoder_params += self.KLD_diag_gaussians(\n",
        "                                        self.decoder.state_dict(keep_vars=True)['last_hidden_layer_'+param_type+'_mean'].flatten(),\n",
        "                                        self.decoder.state_dict(keep_vars=True)['last_hidden_layer_'+param_type+'_log_var'].flatten(),\n",
        "                                        zero_tensor,\n",
        "                                        zero_tensor\n",
        "                )\n",
        "\n",
        "        if self.decoder.include_sparsity:\n",
        "            self.logit_scale_sigma = 4.0\n",
        "            self.logit_scale_mu = 2.0**0.5 * self.logit_scale_sigma * erfinv(2.0 * self.logit_sparsity_p - 1.0)\n",
        "\n",
        "            sparsity_mu = torch.tensor(self.logit_scale_mu).to(self.device) \n",
        "            sparsity_log_var = torch.log(torch.tensor(self.logit_scale_sigma**2)).to(self.device)\n",
        "            KLD_decoder_params += self.KLD_diag_gaussians(\n",
        "                                    self.decoder.state_dict(keep_vars=True)['sparsity_weight_mean'].flatten(),\n",
        "                                    self.decoder.state_dict(keep_vars=True)['sparsity_weight_log_var'].flatten(),\n",
        "                                    sparsity_mu,\n",
        "                                    sparsity_log_var\n",
        "            )\n",
        "            \n",
        "        if self.decoder.convolve_output:\n",
        "            for param_type in ['weight']:\n",
        "                KLD_decoder_params += self.KLD_diag_gaussians(\n",
        "                                    self.decoder.state_dict(keep_vars=True)['output_convolution_mean.'+param_type].flatten(),\n",
        "                                    self.decoder.state_dict(keep_vars=True)['output_convolution_log_var.'+param_type].flatten(),\n",
        "                                    zero_tensor,\n",
        "                                    zero_tensor\n",
        "                )\n",
        "\n",
        "        if self.decoder.include_temperature_scaler:\n",
        "            KLD_decoder_params += self.KLD_diag_gaussians(\n",
        "                                    self.decoder.state_dict(keep_vars=True)['temperature_scaler_mean'].flatten(),\n",
        "                                    self.decoder.state_dict(keep_vars=True)['temperature_scaler_log_var'].flatten(),\n",
        "                                    zero_tensor,\n",
        "                                    zero_tensor\n",
        "            )        \n",
        "        return KLD_decoder_params\n",
        "\n",
        "    def loss_function(self, x_recon_log, x, mu, log_var, kl_latent_scale, kl_global_params_scale, annealing_warm_up, training_step, Neff):\n",
        "        \"\"\"\n",
        "        Returns mean of negative ELBO, reconstruction loss and KL divergence across batch x.\n",
        "        \"\"\"\n",
        "        BCE = F.binary_cross_entropy_with_logits(x_recon_log, x, reduction='sum') / x.shape[0]\n",
        "        KLD_latent = (-0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())) / x.shape[0]\n",
        "        if self.decoder.bayesian_decoder:\n",
        "            KLD_decoder_params_normalized = self.KLD_global_parameters() / Neff\n",
        "        else:\n",
        "            KLD_decoder_params_normalized = 0.0\n",
        "        warm_up_scale = self.annealing_factor(annealing_warm_up,training_step)\n",
        "        neg_ELBO = BCE + warm_up_scale * (kl_latent_scale * KLD_latent + kl_global_params_scale * KLD_decoder_params_normalized)\n",
        "        return neg_ELBO, BCE, KLD_latent, KLD_decoder_params_normalized\n",
        "    \n",
        "    def all_likelihood_components(self, x):\n",
        "        \"\"\"\n",
        "        Returns tensors of ELBO, reconstruction loss and KL divergence for each point in batch x.\n",
        "        \"\"\"\n",
        "        mu, log_var = self.encoder(x)\n",
        "        z = self.sample_latent(mu, log_var)\n",
        "        recon_x_log = self.decoder(z)\n",
        "\n",
        "        recon_x_log = recon_x_log.view(-1,self.alphabet_size*self.seq_len)\n",
        "        x = x.view(-1,self.alphabet_size*self.seq_len)\n",
        "        \n",
        "        BCE_batch_tensor = torch.sum(F.binary_cross_entropy_with_logits(recon_x_log, x, reduction='none'),dim=1)\n",
        "        KLD_batch_tensor = (-0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(),dim=1))\n",
        "        \n",
        "        ELBO_batch_tensor = -(BCE_batch_tensor + KLD_batch_tensor)\n",
        "\n",
        "        return ELBO_batch_tensor, BCE_batch_tensor, KLD_batch_tensor\n",
        "\n",
        "    def train_model(self, data, training_parameters):\n",
        "        \"\"\"\n",
        "        Training procedure for the VAE model.\n",
        "        If use_validation_set is True then:\n",
        "            - we split the alignment data in train/val sets.\n",
        "            - we train up to num_training_steps steps but store the version of the model with lowest loss on validation set across training\n",
        "        If not, then we train the model for num_training_steps and save the model at the end of training\n",
        "        \"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            cudnn.benchmark = True\n",
        "        self.train()\n",
        "        \n",
        "        # if training_parameters['log_training_info']:\n",
        "        #     filename = training_parameters['training_logs_location']+os.sep+self.model_name+\"_losses.csv\"\n",
        "        #     with open(filename, \"a\") as logs:\n",
        "        #         logs.write(\"Number of sequences in alignment file:\\t\"+str(data.num_sequences)+\"\\n\")\n",
        "        #         logs.write(\"Neff:\\t\"+str(self.Neff)+\"\\n\")\n",
        "        #         logs.write(\"Alignment sequence length:\\t\"+str(data.seq_len)+\"\\n\")\n",
        "\n",
        "        optimizer = optim.Adam(self.parameters(), lr=training_parameters['learning_rate'], weight_decay = training_parameters['l2_regularization'])\n",
        "        \n",
        "        if training_parameters['use_lr_scheduler']:\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=training_parameters['lr_scheduler_step_size'], gamma=training_parameters['lr_scheduler_gamma'])\n",
        "\n",
        "        if training_parameters['use_validation_set']:\n",
        "            x_train, x_val, weights_train, weights_val = train_test_split(data.one_hot_encoding, data.weights, test_size=training_parameters['validation_set_pct'], random_state=self.random_seed)\n",
        "            best_val_loss = float('inf')\n",
        "            best_model_step_index=0\n",
        "        else:\n",
        "            x_train = data.one_hot_encoding\n",
        "            weights_train = data.weights\n",
        "            best_val_loss = None\n",
        "            best_model_step_index = training_parameters['num_training_steps']\n",
        "\n",
        "        batch_order = np.arange(x_train.shape[0])\n",
        "        # print(weights_train)\n",
        "        # print(torch.sum(weights_train))\n",
        "        seq_sample_probs = weights_train / np.sum(weights_train)\n",
        "\n",
        "        self.Neff_training = np.sum(weights_train)\n",
        "        N_training =  x_train.shape[0]\n",
        "        \n",
        "        start = time.time()\n",
        "        train_loss = 0\n",
        "        \n",
        "        print(weights_train)\n",
        "\n",
        "        for training_step in tqdm.tqdm(range(1,training_parameters['num_training_steps']+1), desc=\"Training model\", leave=False, ascii=True):\n",
        "\n",
        "\n",
        "            batch_index = np.random.choice(batch_order, training_parameters['batch_size'], p=seq_sample_probs).tolist()\n",
        "            # print(len(batch_index))\n",
        "            x = torch.tensor(x_train[batch_index], dtype=self.dtype).to(self.device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            mu, log_var = self.encoder(x)\n",
        "            z = self.sample_latent(mu, log_var)\n",
        "            recon_x_log = self.decoder(z)\n",
        "            \n",
        "            neg_ELBO, BCE, KLD_latent, KLD_decoder_params_normalized = self.loss_function(recon_x_log, x, mu, log_var, training_parameters['kl_latent_scale'], training_parameters['kl_global_params_scale'], training_parameters['annealing_warm_up'], training_step, self.Neff_training)\n",
        "            \n",
        "            neg_ELBO.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if training_parameters['use_lr_scheduler']:\n",
        "                scheduler.step()\n",
        "            \n",
        "            if training_step % training_parameters['log_training_freq'] == 0:\n",
        "                progress = \"|Train : Update {0}. Negative ELBO : {1:.3f}, BCE: {2:.3f}, KLD_latent: {3:.3f}, KLD_decoder_params_norm: {4:.3f}, Time: {5:.2f} |\".format(training_step, neg_ELBO, BCE, KLD_latent, KLD_decoder_params_normalized, time.time() - start)\n",
        "                print(progress)\n",
        "                mut_scores_list = []\n",
        "                mean_mut_score = self.get_mutation_likelihood(wt_one_hot, mut_one_hot, test_dataloader, 50, training_parameters['batch_size'])\n",
        "\n",
        "                print(spearmanr(mean_mut_score, exp_list))\n",
        "\n",
        "                z_list, label_list = model.get_z(dataloader)\n",
        "                self.visualize_z(z_list, label_list, training_step, 1, 1)\n",
        "\n",
        "                if training_parameters['log_training_info']:\n",
        "                    with open(filename, \"a\") as logs:\n",
        "                        logs.write(progress+\"\\n\")\n",
        "\n",
        "            if training_step % training_parameters['save_model_params_freq']==0:\n",
        "                self.save(model_checkpoint=training_parameters['model_checkpoint_location']+os.sep+self.model_name+\"_step_\"+str(training_step),\n",
        "                            encoder_parameters=encoder_parameters,\n",
        "                            decoder_parameters=decoder_parameters,\n",
        "                            training_parameters=training_parameters)\n",
        "            \n",
        "            if training_parameters['use_validation_set'] and training_step % training_parameters['validation_freq'] == 0:\n",
        "                x_val = torch.tensor(x_val, dtype=self.dtype).to(self.device)\n",
        "                val_neg_ELBO, val_BCE, val_KLD_latent, val_KLD_global_parameters = self.test_model(x_val, weights_val, training_parameters['batch_size'])\n",
        "\n",
        "                progress_val = \"\\t\\t\\t|Val : Update {0}. Negative ELBO : {1:.3f}, BCE: {2:.3f}, KLD_latent: {3:.3f}, KLD_decoder_params_norm: {4:.3f}, Time: {5:.2f} |\".format(training_step, val_neg_ELBO, val_BCE, val_KLD_latent, val_KLD_global_parameters, time.time() - start)\n",
        "                print(progress_val)\n",
        "                if training_parameters['log_training_info']:\n",
        "                    with open(filename, \"a\") as logs:\n",
        "                        logs.write(progress_val+\"\\n\")\n",
        "\n",
        "                if val_neg_ELBO < best_val_loss:\n",
        "                    best_val_loss = val_neg_ELBO\n",
        "                    best_model_step_index = training_step\n",
        "                    self.save(model_checkpoint=training_parameters['model_checkpoint_location']+os.sep+self.model_name+\"_best\",\n",
        "                                encoder_parameters=encoder_parameters,\n",
        "                                decoder_parameters=decoder_parameters,\n",
        "                                training_parameters=training_parameters)\n",
        "                self.train()\n",
        "    \n",
        "    def test_model(self, x_val, weights_val, batch_size):\n",
        "        self.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            val_batch_order = np.arange(x_val.shape[0])\n",
        "            val_seq_sample_probs = weights_val / np.sum(weights_val)\n",
        "\n",
        "            val_batch_index = np.random.choice(val_batcneffh_order, batch_size, p=val_seq_sample_probs).tolist()\n",
        "            x = torch.tensor(x_val[val_batch_index], dtype=self.dtype).to(self.device)\n",
        "            mu, log_var = self.encoder(x)\n",
        "            z = self.sample_latent(mu, log_var)\n",
        "            recon_x_log = self.decoder(z)\n",
        "            \n",
        "            neg_ELBO, BCE, KLD_latent, KLD_global_parameters = self.loss_function(recon_x_log, x, mu, log_var, kl_latent_scale=1.0, kl_global_params_scale=1.0, annealing_warm_up=0, training_step=1, Neff = self.Neff_training) #set annealing factor to 1\n",
        "            \n",
        "        return neg_ELBO.item(), BCE.item(), KLD_latent.item(), KLD_global_parameters.item()\n",
        "\n",
        "\n",
        "    def get_z(self, dataloader):\n",
        "        z_list = []\n",
        "        label_list = []\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for x,y in dataloader:\n",
        "                # For illustrative purposes, make sure we can see the entire tensor\n",
        "                torch.set_printoptions(threshold=np.inf)\n",
        "                # print(\"data: \", x)\n",
        "                # print(\"labels: \", y)\n",
        "                x = torch.tensor(x, dtype=self.dtype).to(self.device)\n",
        "\n",
        "                mu, log_var = self.encoder(x)\n",
        "                z = self.sample_latent(mu, log_var)\n",
        "                # interrupt after first batch\n",
        "                z_list.append(mu)\n",
        "                label_list.append(y)\n",
        "\n",
        "        label_list = torch.cat(label_list, dim=0).cpu().numpy()\n",
        "        z_list = torch.cat(z_list, dim=0).cpu().numpy()\n",
        "        return(z_list, label_list)\n",
        "\n",
        "    def get_mutation_likelihood(self, wt_sequence, mut_one_hot_df, mut_dataloader, num_samples, batch_size=256):\n",
        "        \n",
        "        Neff_training = np.sum(np.ones(len(mut_one_hot_df)))\n",
        "        self.eval()\n",
        "\n",
        "        prediction_matrix_wt = torch.zeros((1,num_samples))\n",
        "        with torch.no_grad():\n",
        "            x = wt_sequence.to(self.device)\n",
        "            for i in range(num_samples):\n",
        "                seq_predictions_wt, _, _ = self.all_likelihood_components(x)\n",
        "                prediction_matrix_wt[0,i] = seq_predictions_wt\n",
        "\n",
        "        mean_wt_score = prediction_matrix_wt.mean(dim=1, keepdim=False)[0]\n",
        "\n",
        "\n",
        "        prediction_matrix = torch.zeros((len(mut_one_hot_df),num_samples))\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # for i, batch in enumerate(tqdm.tqdm(mut_dataloader, 'Looping through mutation batches')):\n",
        "            for i, batch in enumerate(tqdm.auto.tqdm(mut_dataloader, 'Looping through mutation batches', leave=True, ascii=True)):\n",
        "                # print(batch[0])\n",
        "                # print(batch[1])\n",
        "                x = batch.type(self.dtype).to(self.device)\n",
        "                for j in range(num_samples):\n",
        "                    seq_predictions, _, _ = self.all_likelihood_components(x)\n",
        "                    prediction_matrix[i*batch_size:i*batch_size+len(x),j] = seq_predictions\n",
        "            mean_predictions = prediction_matrix.mean(dim=1, keepdim=False)\n",
        "            # std_predictions = prediction_matrix.std(dim=1, keepdim=False)\n",
        "            delta_elbos =   mean_wt_score - mean_predictions\n",
        "            evol_indices = - delta_elbos.detach().cpu().numpy()\n",
        "        return(evol_indices)\n",
        "\n",
        "    def visualize_z(self, z_list, label_list, iteration, s, alpha):\n",
        "        cdict = {0:'tab:cyan', 1: 'tab:blue', 2: 'tab:orange', 3: 'tab:green', 4: 'tab:red', 5:'tab:purple', 6:'tab:brown', 7:'tab:pink' ,8:'tab:gray', 9: 'tab:olive'}\n",
        "        fig, ax = plt.subplots()\n",
        "        for g in np.unique(label_list):\n",
        "            ix = np.where(label_list == g)\n",
        "            ax.scatter(z_list[ix,0], z_list[ix,1], c = cdict[g], marker='.', label = phyla_lookup_table[g], s=s, alpha=alpha)\n",
        "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.65))\n",
        "        plt.xlabel('values of Z1')\n",
        "        plt.ylabel('values of Z2')\n",
        "        plt.title('latent space of dimension 2 of VAE model')\n",
        "        plt.show()\n",
        "        plt.savefig(f\"{iteration}.png\")\n",
        "\n",
        "    def save(self, model_checkpoint, encoder_parameters, decoder_parameters, training_parameters, batch_size=256):\n",
        "        torch.save({\n",
        "            'model_state_dict':self.state_dict(),\n",
        "            'encoder_parameters':encoder_parameters,\n",
        "            'decoder_parameters':decoder_parameters,\n",
        "            'training_parameters':training_parameters,\n",
        "            }, model_checkpoint)\n",
        "    \n",
        "    def compute_evol_indices(self, msa_data, list_mutations_location, num_samples, batch_size=256):\n",
        "        \"\"\"\n",
        "        The column in the list_mutations dataframe that contains the mutant(s) for a given variant should be called \"mutations\"\n",
        "        \"\"\"\n",
        "        #Multiple mutations are to be passed colon-separated\n",
        "        list_mutations=pd.read_csv(list_mutations_location, header=0)\n",
        "        \n",
        "        #Remove (multiple) mutations that are invalid\n",
        "        list_valid_mutations = ['wt']\n",
        "        list_valid_mutated_sequences = {}\n",
        "        list_valid_mutated_sequences['wt'] = msa_data.focus_seq_trimmed # first sequence in the list is the wild_type\n",
        "        for mutation in list_mutations['mutations']:\n",
        "            individual_substitutions = mutation.split(':')\n",
        "            mutated_sequence = list(msa_data.focus_seq_trimmed)[:]\n",
        "            fully_valid_mutation = True\n",
        "            for mut in individual_substitutions:\n",
        "                wt_aa, pos, mut_aa = mut[0], int(mut[1:-1]), mut[-1]\n",
        "                if pos not in msa_data.uniprot_focus_col_to_wt_aa_dict or msa_data.uniprot_focus_col_to_wt_aa_dict[pos] != wt_aa or mut not in msa_data.mutant_to_letter_pos_idx_focus_list:\n",
        "                    print (\"Not a valid mutant: \"+mutation)\n",
        "                    fully_valid_mutation = False\n",
        "                    break\n",
        "                else:\n",
        "                    wt_aa,pos,idx_focus = msa_data.mutant_to_letter_pos_idx_focus_list[mut]\n",
        "                    mutated_sequence[idx_focus] = mut_aa #perform the corresponding AA substitution\n",
        "            \n",
        "            if fully_valid_mutation:\n",
        "                list_valid_mutations.append(mutation)\n",
        "                list_valid_mutated_sequences[mutation] = ''.join(mutated_sequence)\n",
        "        \n",
        "        #One-hot encoding of mutated sequences\n",
        "        mutated_sequences_one_hot = np.zeros((len(list_valid_mutations),len(msa_data.focus_cols),len(msa_data.alphabet)))\n",
        "        for i,mutation in enumerate(list_valid_mutations):\n",
        "            sequence = list_valid_mutated_sequences[mutation]\n",
        "            for j,letter in enumerate(sequence):\n",
        "                if letter in msa_data.aa_dict:\n",
        "                    k = msa_data.aa_dict[letter]\n",
        "                    mutated_sequences_one_hot[i,j,k] = 1.0\n",
        "\n",
        "        mutated_sequences_one_hot = torch.tensor(mutated_sequences_one_hot)\n",
        "        dataloader = torch.utils.data.DataLoader(mutated_sequences_one_hot, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        prediction_matrix = torch.zeros((len(list_valid_mutations),num_samples))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(tqdm.tqdm(dataloader, 'Looping through mutation batches')):\n",
        "                x = batch.type(self.dtype).to(self.device)\n",
        "                for j in range(num_samples):\n",
        "                    seq_predictions, _, _ = self.all_likelihood_components(x)\n",
        "                    prediction_matrix[i*batch_size:i*batch_size+len(x),j] = seq_predictions\n",
        "                tqdm.tqdm.write('\\n')\n",
        "            mean_predictions = prediction_matrix.mean(dim=1, keepdim=False)\n",
        "            std_predictions = prediction_matrix.std(dim=1, keepdim=False)\n",
        "            delta_elbos = mean_predictions - mean_predictions[0]\n",
        "            evol_indices =  - delta_elbos.detach().cpu().numpy()\n",
        "\n",
        "        return list_valid_mutations, evol_indices, mean_predictions[0].detach().cpu().numpy(), std_predictions.detach().cpu().numpy()"
      ],
      "id": "qhoTmv9HxmCE"
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_class(data_filename='BLAT_ECOLX_1_b0.5_labeled.fasta', calc_weights=model_params[\"training_parameters\"]['calc_weights'])\n",
        "   "
      ],
      "metadata": {
        "id": "WniRnx3_Isnw"
      },
      "id": "WniRnx3_Isnw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pINbe82QDsyT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "c39975a9-11fb-4a30-8a09-f26668e714a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. ... 1. 1. 1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training model:   0%|          | 0/150000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-1716df01fc20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving model: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-ef4cc667aaf6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, data, training_parameters)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_sample_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;31m# print(len(batch_index))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "load_model = False\n",
        "\n",
        "if load_model:\n",
        "    data = data_class(data_filename='BLAT_ECOLX_1_b0.5_labeled.fasta', calc_weights=model_params[\"training_parameters\"]['calc_weights'])\n",
        "    model = VAE_model(\n",
        "                    model_name=model_name,\n",
        "                    data=data,\n",
        "                    encoder_parameters=torch.load(\"experiment1_final\")[\"encoder_parameters\"],\n",
        "                    decoder_parameters=torch.load(\"experiment1_final\")[\"decoder_parameters\"],\n",
        "                    random_seed=42)\n",
        "    model.load_state_dict(torch.load(\"experiment1_final\")['model_state_dict']) # Change name of saved file\n",
        "    model = model.to(model.device)\n",
        "\n",
        "else:\n",
        "\n",
        "    model_name = 'experiment1'\n",
        "\n",
        "    model = VAE_model(\n",
        "                    model_name=model_name,\n",
        "                    data=data,\n",
        "                    encoder_parameters=model_params[\"encoder_parameters\"],\n",
        "                    decoder_parameters=model_params[\"decoder_parameters\"],\n",
        "                    random_seed=42\n",
        "    )\n",
        "    model = model.to(model.device)\n",
        "\n",
        "    model.train_model(data=data, training_parameters=model_params[\"training_parameters\"])\n",
        "\n",
        "    print(\"Saving model: \" + model_name)\n",
        "    model.save(model_checkpoint=model_params[\"training_parameters\"]['model_checkpoint_location']+os.sep+model_name+\"_final\", \n",
        "                encoder_parameters=model_params[\"encoder_parameters\"], \n",
        "                decoder_parameters=model_params[\"decoder_parameters\"], \n",
        "                training_parameters=model_params[\"training_parameters\"]\n",
        "    )"
      ],
      "id": "pINbe82QDsyT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-us7Lf7ddvb"
      },
      "source": [
        "Get sequences for test set"
      ],
      "id": "w-us7Lf7ddvb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXPMvShyW5RX"
      },
      "source": [
        "##Get Z values and mut_scores"
      ],
      "id": "KXPMvShyW5RX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJY6N5BbpX4Y"
      },
      "outputs": [],
      "source": [
        "if __name__=='__main__':\n",
        "    z_list, label_list = model.get_z(dataloader)\n",
        "    mut_scores = model.get_mutation_likelihood(wt_one_hot, mut_one_hot)"
      ],
      "id": "aJY6N5BbpX4Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cWWtFqfHZKj"
      },
      "outputs": [],
      "source": [
        "cdict = {0:'tab:cyan', 1: 'tab:blue', 2: 'tab:orange', 3: 'tab:green', 4: 'tab:red', 5:'tab:purple', 6:'tab:brown', 7:'tab:pink' ,8:'tab:gray', 9: 'tab:olive'}\n",
        "fig, ax = plt.subplots()\n",
        "for g in np.unique(label_list):\n",
        "    ix = np.where(label_list == g)\n",
        "    ax.scatter(z_list[ix,0], z_list[ix,1], c = cdict[g], marker='.', label = phyla_lookup_table[g], alpha=1)\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1, 0.65))\n",
        "plt.xlabel('values of Z1')\n",
        "plt.ylabel('values of Z2')\n",
        "plt.title('latent space of dimension 2 of VAE model')\n",
        "plt.show()"
      ],
      "id": "8cWWtFqfHZKj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QApx9ELrK0--"
      },
      "outputs": [],
      "source": [
        "spear = spearmanr(mut_scores, exp_list)\n",
        "print(spear)"
      ],
      "id": "QApx9ELrK0--"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "protein_vae_data_processing_01_19.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}